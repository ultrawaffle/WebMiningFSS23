{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import surprise as sp\n",
    "\n",
    "rec_path = r'/Users/erik/Downloads/archive(1)/recommendations.csv'\n",
    "games_path = r'/Users/erik/Downloads/archive(1)/games.csv'\n",
    "users_path = r'/Users/erik/Downloads/archive(1)/users.csv'\n",
    "games_metadata_path = r'/Users/erik/Downloads/archive(1)/games_metadata.json'\n",
    "\n",
    "recommendations = pd.read_csv(rec_path)\n",
    "games = pd.read_csv(games_path)\n",
    "users = pd.read_csv(users_path)\n",
    "games_metadata = pd.read_json(games_metadata_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "my_seed = 1\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from surprise import Dataset, SVD\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=0.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "       user_est_true[uid].append((est, true_r))\n",
    "   #     if uid == 922219:\n",
    "   #         print((est, true_r))\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "def get_recommendation_for_user(uid, top_n, testset):\n",
    "    recommended = pd.Series([(x,y) for (x,y) in top_n[uid]])\n",
    "    #print(recommended)\n",
    "\n",
    "    if testset != None:\n",
    "        in_testset = pd.Series([(i, r) for (u, i, r) in testset if u == uid])\n",
    "\n",
    "    print (\"\\nUser: \" + str(uid))\n",
    "    print(\"Rated:\")\n",
    "    counter = 1\n",
    "    if testset != None:\n",
    "        for i in in_testset:\n",
    "            print(str(counter) + \"- \"+ str(i) + '- ' + games[games['app_id'] == i[0]]['title'].iloc[0])\n",
    "            counter+=1\n",
    "\n",
    "    print(\"\\n Recommended Items:\")\n",
    "    counter = 1\n",
    "    for r in recommended:\n",
    "        print(str(counter) + \"- \" + str(r) + '- ' +  games[games['app_id'] == r[0]]['title'].iloc[0])\n",
    "        counter+=1\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def precision_recall_f1(predictions, threshold=3.5):\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    # collect metrics per user\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    f1s = dict()\n",
    "    accuracies = dict()\n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        \n",
    "        # get relevance labels for average precision calculation\n",
    "        y_true = [1 if (true_r >= threshold) else 0 for (_, true_r) in user_ratings]\n",
    "        y_pred = [1 if (est_r >= threshold) else 0 for (est_r, _) in user_ratings]\n",
    "        \n",
    "        precisions[uid] = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recalls[uid] = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1s[uid] = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        accuracies[uid] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # average scores over all users \n",
    "    avg_precision = sum(list(precisions.values())) / len(list(precisions.values()))\n",
    "    avg_recall = sum(list(recalls.values())) / len(list(recalls.values()))\n",
    "    avg_f1 = sum(list(f1s.values())) / len(list(f1s.values()))\n",
    "    \n",
    "    avg_accuracy = sum(list(accuracies.values())) / len(list(accuracies.values()))\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1, avg_accuracy\n",
    "\n",
    "def get_ratings_from_uid(uid, data):\n",
    "    rs = [(i, r) for (u,i,r,d) in data.raw_ratings if u == uid]\n",
    "    for i in rs:\n",
    "        print((games[games['app_id'] == i[0]]['title'].iloc[0]) + \" rated: \" + str(i[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get User-App Ratings\n",
    "\n",
    "Here we filter on users which rated atleast 20 games which should give us enough data to train a model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>products</th>\n",
       "      <th>reviews</th>\n",
       "      <th>app_id</th>\n",
       "      <th>helpful</th>\n",
       "      <th>funny</th>\n",
       "      <th>date</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>hours</th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1965432</td>\n",
       "      <td>702</td>\n",
       "      <td>32</td>\n",
       "      <td>264710</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-05-30</td>\n",
       "      <td>1</td>\n",
       "      <td>33.9</td>\n",
       "      <td>2063767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1965432</td>\n",
       "      <td>702</td>\n",
       "      <td>32</td>\n",
       "      <td>239030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-12</td>\n",
       "      <td>1</td>\n",
       "      <td>10.3</td>\n",
       "      <td>4517660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1965432</td>\n",
       "      <td>702</td>\n",
       "      <td>32</td>\n",
       "      <td>250900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>61.6</td>\n",
       "      <td>5762464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1965432</td>\n",
       "      <td>702</td>\n",
       "      <td>32</td>\n",
       "      <td>607080</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6152399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1965432</td>\n",
       "      <td>702</td>\n",
       "      <td>32</td>\n",
       "      <td>335300</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>6410467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429428</th>\n",
       "      <td>922219</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>1361000</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>2022-06-26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>9366376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429429</th>\n",
       "      <td>922219</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>289650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>1</td>\n",
       "      <td>16.1</td>\n",
       "      <td>9639562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429430</th>\n",
       "      <td>922219</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>424840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>9697813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429431</th>\n",
       "      <td>922219</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>633230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11229597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429432</th>\n",
       "      <td>922219</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>952060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-26</td>\n",
       "      <td>1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>11247949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429433 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  products  reviews   app_id  helpful  funny        date  \\\n",
       "0       1965432       702       32   264710        0      2  2018-05-30   \n",
       "1       1965432       702       32   239030        0      0  2013-12-12   \n",
       "2       1965432       702       32   250900        0      0  2017-02-17   \n",
       "3       1965432       702       32   607080        2      0  2021-12-25   \n",
       "4       1965432       702       32   335300        2      0  2017-01-01   \n",
       "...         ...       ...      ...      ...      ...    ...         ...   \n",
       "429428   922219        72       25  1361000       28     19  2022-06-26   \n",
       "429429   922219        72       25   289650        0      0  2022-06-24   \n",
       "429430   922219        72       25   424840        0      0  2021-12-28   \n",
       "429431   922219        72       25   633230        0      0  2021-12-28   \n",
       "429432   922219        72       25   952060        0      0  2022-06-26   \n",
       "\n",
       "        is_recommended  hours  review_id  \n",
       "0                    1   33.9    2063767  \n",
       "1                    1   10.3    4517660  \n",
       "2                    1   61.6    5762464  \n",
       "3                    1   19.0    6152399  \n",
       "4                    1  164.0    6410467  \n",
       "...                ...    ...        ...  \n",
       "429428               1    1.1    9366376  \n",
       "429429               1   16.1    9639562  \n",
       "429430               0    2.7    9697813  \n",
       "429431               1   10.0   11229597  \n",
       "429432               1    6.2   11247949  \n",
       "\n",
       "[429433 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_app_ratings = pd.merge(users.loc[users['reviews'] >= 20], recommendations, how=\"inner\", on=[\"user_id\"])\n",
    "user_app_ratings['is_recommended'] = user_app_ratings['is_recommended'].map({False: 0, True: 1})\n",
    "user_app_ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "Create dataset object for surprise library and split data into 75/25 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "data = sp.Dataset.load_from_df(user_app_ratings[[\"user_id\", \"app_id\", \"is_recommended\"]], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(922219, 1449850, 0.0),\n",
       " (922219, 311210, 1.0),\n",
       " (922219, 383150, 1.0),\n",
       " (922219, 1361000, 1.0),\n",
       " (922219, 617830, 1.0),\n",
       " (922219, 1240440, 1.0),\n",
       " (922219, 1172470, 0.0),\n",
       " (922219, 349040, 1.0),\n",
       " (922219, 1211630, 1.0),\n",
       " (922219, 438100, 1.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(u, i, r) for (u, i, r) in testset if u == 922219]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train First Model\n",
    "\n",
    "Here we train a KNN Basic model with default settings to get an impression of the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNWithMeans at 0x2a45aabb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import KNNWithMeans\n",
    "\n",
    "# User-based with Pearson correlation similarity and simple prediction\n",
    "sim_options = {'name': 'pearson', 'user_based': True, 'min_support': 1}\n",
    "\n",
    "# Build algorithm\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict a rating for a specific user. As we can see the model predicts the rating as 0.56. Now depending on the threshhold we set we can interpret this as true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 922219     item: 1449850    r_ui = 0.00   est = 0.67   {'actual_k': 10, 'was_impossible': False}\n",
      "user: 922219     item: 607080     r_ui = 0.00   est = 1.00   {'actual_k': 4, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "uid = 922219\n",
    "iid = 1449850\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo.predict(uid, iid, r_ui=0, verbose=True)\n",
    "pred = algo.predict(uid, 607080, r_ui=0, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance\n",
    "\n",
    "Next we try to evaluate the performance of this model. First we use some standard measures to get a first impression of the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1380\n",
      "RMSE: 0.3715\n",
      "MAE:  0.2460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24600086226755605"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.accuracy import mse, rmse, mae\n",
    "# Compute MSE, RMSE and MAE on the test set predictions\n",
    "\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "mse(predictions)\n",
    "rmse(predictions)\n",
    "mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.8042665199786134\n",
      "Avg. Recall: 0.8796142173655607\n",
      "Avg. F1: 0.8255403585658555\n",
      "Avg. Accuracy: 0.8094259608122694\n"
     ]
    }
   ],
   "source": [
    "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions, threshold=0.5)\n",
    "print(f'Avg. Precision: {avg_precision}')\n",
    "print(f'Avg. Recall: {avg_recall}')\n",
    "print(f'Avg. F1: {avg_f1}')\n",
    "print(f'Avg. Accuracy: {avg_accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Since we are dealing with a recommender system purely evaluating on MSE/RMSE/MAE will not help for further comparison. Therefore we calculate the precision and recall for each user @k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5: 0.8103360988447172\n"
     ]
    }
   ],
   "source": [
    "predictions = algo.test(testset)\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=0.6)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print('MAP@5: '+ str(sum(prec for prec in precisions.values()) / len(precisions)))\n",
    "#print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD & SVD++\n",
    "\n",
    "Next we will use a more advanced algorithm namely SVD & SVD++. These rely on matrix factorization to predict ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1286\n",
      "RMSE: 0.3586\n",
      "MAE:  0.2615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26151725408216797"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "algo_svd = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo_svd.fit(trainset)\n",
    "predictions_svd = algo_svd.test(testset)\n",
    "\n",
    "# Then compute MSE, RMSE and MAE\n",
    "mse(predictions_svd)\n",
    "rmse(predictions_svd)\n",
    "mae(predictions_svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.8031573934617847\n",
      "Avg. Recall: 0.9181383471017706\n",
      "Avg. F1: 0.8427892641553852\n",
      "Avg. Accuracy: 0.8210451596413622\n"
     ]
    }
   ],
   "source": [
    "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions_svd, threshold=0.5)\n",
    "print(f'Avg. Precision: {avg_precision}')\n",
    "print(f'Avg. Recall: {avg_recall}')\n",
    "print(f'Avg. F1: {avg_f1}')\n",
    "print(f'Avg. Accuracy: {avg_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10: 0.8271053570631903\n"
     ]
    }
   ],
   "source": [
    "precisions_svd, recalls_svd = precision_recall_at_k(predictions_svd, k=5, threshold=0.5)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print('MAP@10: '+ str(sum(prec for prec in precisions_svd.values()) / len(precisions_svd)))\n",
    "#print(sum(rec for rec in recalls_svd.values()) / len(recalls_svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 922219     item: 1449850    r_ui = 0.00   est = 0.64   {'was_impossible': False}\n",
      "user: 922219     item: 607080     r_ui = 0.00   est = 0.99   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "uid = 922219\n",
    "iid = 1449850\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo_svd.predict(uid, iid, r_ui=0, verbose=True)\n",
    "pred = algo_svd.predict(uid, 607080, r_ui=0, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1262\n",
      "RMSE: 0.3553\n",
      "MAE:  0.2564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2564247476445066"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVDpp\n",
    "\n",
    "algo_svd_pp = SVDpp()\n",
    "\n",
    "algo_svd_pp.fit(trainset)\n",
    "predictions_svdpp = algo_svd_pp.test(testset)\n",
    "\n",
    "# Then compute MSE, RMSE and MAE\n",
    "mse(predictions_svdpp)\n",
    "rmse(predictions_svdpp)\n",
    "mae(predictions_svdpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.7999136201390769\n",
      "Avg. Recall: 0.9102390376002774\n",
      "Avg. F1: 0.8375235276487459\n",
      "Avg. Accuracy: 0.8234863163769046\n"
     ]
    }
   ],
   "source": [
    "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions_svdpp, threshold=0.5)\n",
    "print(f'Avg. Precision: {avg_precision}')\n",
    "print(f'Avg. Recall: {avg_recall}')\n",
    "print(f'Avg. F1: {avg_f1}')\n",
    "print(f'Avg. Accuracy: {avg_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 922219     item: 1449850    r_ui = 0.00   est = 0.74   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "pred = algo_svd_pp.predict(uid, iid, r_ui=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10: 0.8264608144876969\n",
      "0.7286204712855491\n"
     ]
    }
   ],
   "source": [
    "precisions_svdpp, recalls_svdpp = precision_recall_at_k(predictions_svdpp, k=5, threshold=0.5)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print('MAP@10: '+ str(sum(prec for prec in precisions_svdpp.values()) / len(precisions_svdpp)))\n",
    "print(sum(rec for rec in recalls_svdpp.values()) / len(recalls_svdpp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top n-recommendations\n",
    "\n",
    "We can now retrieve the top-n games for this user. If we set a threshhold of 0.5 for the recommendation we can see that the last two games VRChat and Halo Infinite would fall out of the recommendation although they are rated positive. In the testset we have 8 relevant items, 8 recommended items (with threshhold 0.5) but we only have 6 items in the top 10 which are relevant and recommended. This yields a precision of 0.75. A perfect score would be a 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: 922219\n",
      "Rated:\n",
      "1- (1449850, 0.0)- Yu-Gi-Oh! Master Duel\n",
      "2- (311210, 1.0)- Call of Duty®: Black Ops III\n",
      "3- (383150, 1.0)- Dead Island Definitive Edition\n",
      "4- (1361000, 1.0)- In Silence\n",
      "5- (617830, 1.0)- SUPERHOT VR\n",
      "6- (1240440, 1.0)- Halo Infinite\n",
      "7- (1172470, 0.0)- Apex Legends™\n",
      "8- (349040, 1.0)- NARUTO SHIPPUDEN: Ultimate Ninja STORM 4\n",
      "9- (1211630, 1.0)- The Jackbox Party Pack 7\n",
      "10- (438100, 1.0)- VRChat\n",
      "\n",
      " Recommended Items:\n",
      "1- (1211630, 0.9410860463403207)- The Jackbox Party Pack 7\n",
      "2- (349040, 0.8203087719915527)- NARUTO SHIPPUDEN: Ultimate Ninja STORM 4\n",
      "3- (1449850, 0.7369421507548444)- Yu-Gi-Oh! Master Duel\n",
      "4- (383150, 0.7024076979261503)- Dead Island Definitive Edition\n",
      "5- (1172470, 0.6706176848762508)- Apex Legends™\n",
      "6- (617830, 0.6494469049482899)- SUPERHOT VR\n",
      "7- (311210, 0.609374165844687)- Call of Duty®: Black Ops III\n",
      "8- (1361000, 0.5965696204227892)- In Silence\n",
      "9- (1240440, 0.4698828500087759)- Halo Infinite\n",
      "10- (438100, 0.4639576629518599)- VRChat\n"
     ]
    }
   ],
   "source": [
    "get_recommendation_for_user(uid, get_top_n(predictions_svdpp, n=10), testset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_h = data\n",
    "raw_ratings = data_h.raw_ratings\n",
    "\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "threshold = int(0.8 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "data_h.raw_ratings = A_raw_ratings  # data is now the set A\n",
    "trainset_h, testset_h = train_test_split(data_h, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-19 18:04:48,420]\u001b[0m A new study created in memory with name: svd_optimization\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:48,699]\u001b[0m Trial 0 finished with value: 0.6547897009435472 and parameters: {'learning_rate': 0.009107724261696223, 'no_epochs': 14, 'n_factors': 82, 'reg_all': 0.07876759709401744, 'threshold': 0.7281045655652435}. Best is trial 0 with value: 0.6547897009435472.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:49,162]\u001b[0m Trial 1 finished with value: 0.7686790340636493 and parameters: {'learning_rate': 0.006085965832053526, 'no_epochs': 40, 'n_factors': 56, 'reg_all': 0.43597241656445695, 'threshold': 0.5569587173709498}. Best is trial 1 with value: 0.7686790340636493.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:49,455]\u001b[0m Trial 2 finished with value: 0.7749746788208322 and parameters: {'learning_rate': 0.0024933966787298303, 'no_epochs': 14, 'n_factors': 91, 'reg_all': 0.17297749210379243, 'threshold': 0.5980770489576818}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:50,018]\u001b[0m Trial 3 finished with value: 0.5273628658244044 and parameters: {'learning_rate': 0.008953008294054453, 'no_epochs': 60, 'n_factors': 31, 'reg_all': 0.1499289088540135, 'threshold': 0.8027311473071529}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:50,354]\u001b[0m Trial 4 finished with value: 0.34801961725038644 and parameters: {'learning_rate': 0.003897191354896045, 'no_epochs': 35, 'n_factors': 25, 'reg_all': 0.3740417989424511, 'threshold': 0.8496822847860187}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:51,451]\u001b[0m Trial 5 finished with value: 0.7373793912255451 and parameters: {'learning_rate': 0.004091525273111911, 'no_epochs': 97, 'n_factors': 60, 'reg_all': 0.42151065115751984, 'threshold': 0.5980572721059365}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:52,681]\u001b[0m Trial 6 finished with value: 0.5766991843914921 and parameters: {'learning_rate': 0.009212167971783692, 'no_epochs': 55, 'n_factors': 86, 'reg_all': 0.5774268068658428, 'threshold': 0.7786477135807816}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:53,549]\u001b[0m Trial 7 finished with value: 0.6193133962364732 and parameters: {'learning_rate': 0.0031623546964497954, 'no_epochs': 73, 'n_factors': 70, 'reg_all': 0.09595691002357103, 'threshold': 0.7461763821839839}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:54,150]\u001b[0m Trial 8 finished with value: 0.5817767471613625 and parameters: {'learning_rate': 0.005848126145925283, 'no_epochs': 57, 'n_factors': 57, 'reg_all': 0.39856860197120414, 'threshold': 0.7711305947365167}. Best is trial 2 with value: 0.7749746788208322.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:54,624]\u001b[0m Trial 9 finished with value: 0.7843701689855533 and parameters: {'learning_rate': 0.002276063357186329, 'no_epochs': 38, 'n_factors': 70, 'reg_all': 0.4258686417545064, 'threshold': 0.5514599187820173}. Best is trial 9 with value: 0.7843701689855533.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:54,928]\u001b[0m Trial 10 finished with value: 0.7524574870728715 and parameters: {'learning_rate': 0.002095288096963399, 'no_epochs': 30, 'n_factors': 43, 'reg_all': 0.25714018456240356, 'threshold': 0.6538290689211045}. Best is trial 9 with value: 0.7843701689855533.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:55,096]\u001b[0m Trial 11 finished with value: 0.783906391598699 and parameters: {'learning_rate': 0.0020452594406976336, 'no_epochs': 6, 'n_factors': 98, 'reg_all': 0.21618871948185775, 'threshold': 0.5128678415787654}. Best is trial 9 with value: 0.7843701689855533.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:55,231]\u001b[0m Trial 12 finished with value: 0.7889706274321657 and parameters: {'learning_rate': 0.0020017081984642403, 'no_epochs': 7, 'n_factors': 10, 'reg_all': 0.26948125732400396, 'threshold': 0.5157022268893682}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:55,417]\u001b[0m Trial 13 finished with value: 0.7876805799882721 and parameters: {'learning_rate': 0.0031219453424180866, 'no_epochs': 22, 'n_factors': 12, 'reg_all': 0.2953628442577453, 'threshold': 0.5096701775762544}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:55,602]\u001b[0m Trial 14 finished with value: 0.786934271549656 and parameters: {'learning_rate': 0.003717816385147676, 'no_epochs': 22, 'n_factors': 11, 'reg_all': 0.2865278114093068, 'threshold': 0.5029317950278438}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:55,712]\u001b[0m Trial 15 finished with value: 0.7574204381896689 and parameters: {'learning_rate': 0.00492106712197906, 'no_epochs': 6, 'n_factors': 11, 'reg_all': 0.2905586985532162, 'threshold': 0.6697616249124769}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:55,934]\u001b[0m Trial 16 finished with value: 0.7721733567887412 and parameters: {'learning_rate': 0.0031405604138737076, 'no_epochs': 23, 'n_factors': 25, 'reg_all': 0.34595895087866696, 'threshold': 0.6282535188970011}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:56,570]\u001b[0m Trial 17 finished with value: 0.6772189349112425 and parameters: {'learning_rate': 0.004820928287842935, 'no_epochs': 73, 'n_factors': 36, 'reg_all': 0.22087013880159925, 'threshold': 0.6872968485400599}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:56,890]\u001b[0m Trial 18 finished with value: 0.7756836718375179 and parameters: {'learning_rate': 0.002982433703541674, 'no_epochs': 44, 'n_factors': 18, 'reg_all': 0.02406105478045134, 'threshold': 0.5397241563263212}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n",
      "\u001b[32m[I 2023-05-19 18:04:57,173]\u001b[0m Trial 19 finished with value: 0.780532011301242 and parameters: {'learning_rate': 0.0029336053312241962, 'no_epochs': 24, 'n_factors': 44, 'reg_all': 0.29577882000329075, 'threshold': 0.5909475268210047}. Best is trial 12 with value: 0.7889706274321657.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0020017081984642403, 'no_epochs': 7, 'n_factors': 10, 'reg_all': 0.26948125732400396, 'threshold': 0.5157022268893682}\n",
      "0.7889706274321657\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    lr_all = trial.suggest_float('learning_rate', 0.002, 0.01)\n",
    "    no_epochs = trial.suggest_int('no_epochs', 5, 100)\n",
    "    n_factors = trial.suggest_int('n_factors', 10, 100)\n",
    "    reg_all = trial.suggest_float(\"reg_all\", 0.02, 0.6)\n",
    "    t = trial.suggest_float(\"threshold\", 0.5, 0.9)\n",
    "\n",
    "    algo_svd_pp = SVDpp(n_epochs = no_epochs, lr_all=lr_all, reg_all=reg_all, n_factors=n_factors)\n",
    "    algo_svd_pp.fit(trainset_h)\n",
    "    predictions = algo_svd_pp.test(testset_h)\n",
    "    \n",
    "    precisions, recalls= precision_recall_at_k(predictions, k=5, threshold=t)\n",
    "    \n",
    "    if len(precisions) > 0:\n",
    "        score = (sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    else:\n",
    "        score = 0\n",
    "        \n",
    "    #score = rmse(predictions)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(study_name=\"svd_optimization\",\n",
    "                            direction=\"maximize\",\n",
    "                            sampler=TPESampler())\n",
    "\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased P@k on A,   0.9053490238031303\n",
      "Unbiased P@k on B, 0.8145437028074586\n"
     ]
    }
   ],
   "source": [
    "# retrain on the whole set A\n",
    "trainset_h = data_h.build_full_trainset()\n",
    "\n",
    "algo_svd_ppt = SVDpp(n_epochs = 7, lr_all=0.0020017081984642403, reg_all=0.26948125732400396, n_factors=10)\n",
    "algo_svd_ppt.fit(trainset_h)\n",
    "\n",
    "# Compute biased accuracy on A\n",
    "predictions_h = algo_svd_ppt.test(trainset_h.build_testset())\n",
    "print(\"Biased P@k on A,\", end=\"   \")\n",
    "#accuracy.rmse(predictions)\n",
    "precisions, recalls= precision_recall_at_k(predictions_h, k=5, threshold=0.5)\n",
    "print(str(sum(prec for prec in precisions.values()) / len(precisions)))\n",
    "\n",
    "# Compute unbiased accuracy on B\n",
    "testset_h = data_h.construct_testset(B_raw_ratings)  # testset is now the set B\n",
    "predictions_h = algo_svd_ppt.test(testset_h)\n",
    "print(\"Unbiased P@k on B,\", end=\" \")\n",
    "precisions, recalls= precision_recall_at_k(predictions_h, k=5, threshold=0.5)\n",
    "print(str(sum(prec for prec in precisions.values()) / len(precisions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5: 0.8142446139435174\n"
     ]
    }
   ],
   "source": [
    "algo_svd_ppt2 = SVDpp(n_epochs = 7, lr_all=0.0020017081984642403, reg_all=0.26948125732400396, n_factors=10)\n",
    "algo_svd_ppt2.fit(trainset)\n",
    "predictions_svdpp2 = algo_svd_ppt2.test(testset)\n",
    "\n",
    "precisions_svdpp2, recalls_svdpp2 = precision_recall_at_k(predictions_svdpp, k=5, threshold=0.6)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print('MAP@5: '+ str(sum(prec for prec in precisions_svdpp2.values()) / len(precisions_svdpp2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.7939981175480904\n",
      "Avg. Recall: 0.981415543387114\n",
      "Avg. F1: 0.8592403240757416\n",
      "Avg. Accuracy: 0.7967525062582209\n"
     ]
    }
   ],
   "source": [
    "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions_svdpp2, threshold=0.5)\n",
    "print(f'Avg. Precision: {avg_precision}')\n",
    "print(f'Avg. Recall: {avg_recall}')\n",
    "print(f'Avg. F1: {avg_f1}')\n",
    "print(f'Avg. Accuracy: {avg_accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending New Games\n",
    "\n",
    "With the trained model we can also recommend new games for a user which the user has not played/rated yet. Now, since we don't have a rating for that item/user pair we can not evaluate statistically whether the recommendation was good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_new = trainset.build_anti_testset()\n",
    "predictions_new = algo_svd_ppt2.test(testset_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: 922219\n",
      "Rated:\n",
      "\n",
      " Recommended Items:\n",
      "1- (413150, 0.9418938476654373)- Stardew Valley\n",
      "2- (548430, 0.9414624414550479)- Deep Rock Galactic\n",
      "3- (400, 0.9410470460729086)- Portal\n",
      "4- (105600, 0.9391318776211085)- Terraria\n",
      "5- (1217060, 0.9378025479237493)- Gunfire Reborn\n"
     ]
    }
   ],
   "source": [
    "get_recommendation_for_user(uid, get_top_n(predictions_new, n=5), None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
